{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html, https://download.pytorch.org/whl/cu113/torch_stable.html, https://www.shi-labs.com/natten/wheels/cu113/torch1.11/index.html\n",
      "Collecting git+https://github.com/rwightman/pytorch-image-models.git@9d6aad44f8fd32e89e5cca503efe3ada5071cc2a (from -r requirements.txt (line 8))\n",
      "  Cloning https://github.com/rwightman/pytorch-image-models.git (to revision 9d6aad44f8fd32e89e5cca503efe3ada5071cc2a) to /tmp/pip-req-build-gzk41xwd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-gzk41xwd\n",
      "  Running command git rev-parse -q --verify 'sha^9d6aad44f8fd32e89e5cca503efe3ada5071cc2a'\n",
      "  Running command git fetch -q https://github.com/rwightman/pytorch-image-models.git 9d6aad44f8fd32e89e5cca503efe3ada5071cc2a\n",
      "  Running command git checkout -q 9d6aad44f8fd32e89e5cca503efe3ada5071cc2a\n",
      "  Resolved https://github.com/rwightman/pytorch-image-models.git to commit 9d6aad44f8fd32e89e5cca503efe3ada5071cc2a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==1.11.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp38-cp38-linux_x86_64.whl (22.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting natten==0.14.4\n",
      "  Downloading https://www.shi-labs.com/natten/wheels/cu113/torch1.11/natten-0.14.4%2Btorch1110cu113-cp38-cp38-linux_x86_64.whl (39.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.1/39.1 MB\u001b[0m \u001b[31m392.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting fvcore==0.1.5.post20220305\n",
      "  Downloading fvcore-0.1.5.post20220305.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml==6.0\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/server1-ailab/.local/lib/python3.8/site-packages (from torch==1.11.0+cu113->-r requirements.txt (line 2)) (4.5.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/server1-ailab/.local/lib/python3.8/site-packages (from natten==0.14.4->-r requirements.txt (line 7)) (23.1)\n",
      "Collecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tqdm in /home/server1-ailab/.local/lib/python3.8/site-packages (from fvcore==0.1.5.post20220305->-r requirements.txt (line 9)) (4.65.0)\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting iopath>=0.1.7\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.9/195.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: timm, fvcore, iopath\n",
      "  Building wheel for timm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for timm: filename=timm-0.5.0-py3-none-any.whl size=423339 sha256=48ff0531fce3f1783eb4098db3e50abce08263db21db9fb7c7934e7fe67aaa70\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-916pwpp9/wheels/de/a2/f5/e8471f1549370c7e0fbb38c8446320a5b3cba8441825a31c5d\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220305-py3-none-any.whl size=61190 sha256=f0b701d9e1a3a991f1bf4020e883c7bc56b7b8e0d5aa25acac29e4521feb429d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-916pwpp9/wheels/37/ee/c7/dcea1dee3f1aec9a6b97795824f2462883a7d213cd1efb453e\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=3cf7011358f53de1d6ae0060802919cd87cf921e98c81a104168ee462e880f75\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-916pwpp9/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
      "Successfully built timm fvcore iopath\n",
      "Installing collected packages: urllib3, torch, termcolor, tabulate, pyyaml, portalocker, pillow, numpy, natten, idna, charset-normalizer, certifi, yacs, requests, iopath, torchvision, fvcore, timm\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0+cu118\n",
      "    Uninstalling torch-2.0.0+cu118:\n",
      "      Successfully uninstalled torch-2.0.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.1+cu118\n",
      "    Uninstalling torchvision-0.15.1+cu118:\n",
      "      Successfully uninstalled torchvision-0.15.1+cu118\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.6.13\n",
      "    Uninstalling timm-0.6.13:\n",
      "      Successfully uninstalled timm-0.6.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2022.12.7 charset-normalizer-3.1.0 fvcore-0.1.5.post20220305 idna-3.4 iopath-0.1.10 natten-0.14.4+torch1110cu113 numpy-1.24.3 pillow-9.5.0 portalocker-2.7.0 pyyaml-6.0 requests-2.29.0 tabulate-0.9.0 termcolor-2.3.0 timm-0.5.0 torch-1.11.0+cu113 torchvision-0.12.0+cu113 urllib3-1.26.15 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# %cd ...\n",
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'small_dog_cat_dataset'...\n",
      "remote: Enumerating objects: 2608, done.\u001b[K\n",
      "remote: Total 2608 (delta 0), reused 0 (delta 0), pack-reused 2608\u001b[K\n",
      "Receiving objects: 100% (2608/2608), 55.84 MiB | 3.38 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anminhhung/small_dog_cat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO CONFIG FILE FOUND, using defaults!\n",
      "Training with a single process on 1 GPUs.\n",
      "---------USE MY MODEL\n",
      "************************************************************\n",
      "Downloading: \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_base_in1k_224.pth\" to /home/server1-ailab/.cache/torch/hub/checkpoints/dinat_base_in1k_224.pth\n",
      "100%|████████████████████████████████████████| 343M/343M [04:40<00:00, 1.28MB/s]\n",
      "WARNING: Unsupported operator aten::mul encountered 148 time(s)\n",
      "WARNING: Unsupported operator aten::softmax encountered 30 time(s)\n",
      "WARNING: Unsupported operator aten::add encountered 118 time(s)\n",
      "WARNING: Unsupported operator aten::gelu encountered 30 time(s)\n",
      "WARNING: Unsupported operator aten::rand encountered 58 time(s)\n",
      "WARNING: Unsupported operator aten::floor_ encountered 58 time(s)\n",
      "WARNING: Unsupported operator aten::div encountered 58 time(s)\n",
      "WARNING: Unsupported operator aten::adaptive_avg_pool1d encountered 1 time(s)\n",
      "Model dinat_base created.\n",
      "89.770M Params and 13.728GFLOPs\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.875\n",
      "AMP not enabled. Training in float32.\n",
      "Scheduled epochs: 2\n",
      "Train: 0 [   0/500 (  0%)]  Loss: 9.102 (9.10)  Time: 0.778s,    5.14/s  (0.778s,    5.14/s)  LR: 1.100e-06  Data: 0.423 (0.423)\n",
      "Train: 0 [  50/500 ( 10%)]  Loss: 7.422 (8.37)  Time: 0.134s,   29.79/s  (0.145s,   27.58/s)  LR: 6.095e-06  Data: 0.002 (0.010)\n",
      "Train: 0 [ 100/500 ( 20%)]  Loss: 3.340 (6.68)  Time: 0.123s,   32.50/s  (0.136s,   29.35/s)  LR: 1.109e-05  Data: 0.002 (0.006)\n",
      "Train: 0 [ 150/500 ( 30%)]  Loss: 1.605 (5.08)  Time: 0.133s,   30.09/s  (0.135s,   29.71/s)  LR: 1.608e-05  Data: 0.002 (0.005)\n",
      "Train: 0 [ 200/500 ( 40%)]  Loss: 1.196 (4.22)  Time: 0.135s,   29.69/s  (0.135s,   29.69/s)  LR: 2.108e-05  Data: 0.002 (0.004)\n",
      "Train: 0 [ 250/500 ( 50%)]  Loss: 1.566 (3.69)  Time: 0.130s,   30.75/s  (0.135s,   29.66/s)  LR: 2.607e-05  Data: 0.002 (0.004)\n",
      "Train: 0 [ 300/500 ( 60%)]  Loss: 1.221 (3.32)  Time: 0.137s,   29.16/s  (0.135s,   29.66/s)  LR: 3.107e-05  Data: 0.002 (0.003)\n",
      "Train: 0 [ 350/500 ( 70%)]  Loss: 1.473 (3.05)  Time: 0.138s,   29.08/s  (0.135s,   29.67/s)  LR: 3.606e-05  Data: 0.002 (0.003)\n",
      "Train: 0 [ 400/500 ( 80%)]  Loss: 1.606 (2.85)  Time: 0.140s,   28.49/s  (0.135s,   29.69/s)  LR: 4.106e-05  Data: 0.002 (0.003)\n",
      "Train: 0 [ 450/500 ( 90%)]  Loss: 1.398 (2.69)  Time: 0.137s,   29.09/s  (0.135s,   29.65/s)  LR: 4.605e-05  Data: 0.002 (0.003)\n",
      "Train: 0 [ 499/500 (100%)]  Loss: 1.089 (2.57)  Time: 0.124s,   32.24/s  (0.135s,   29.68/s)  LR: 5.095e-05  Data: 0.000 (0.003)\n",
      "Test: [   0/649]  Time: 0.486 (0.486)  Loss:  0.1726 (0.1726)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/649]  Time: 0.018 (0.027)  Loss:  0.2131 (0.1870)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/649]  Time: 0.018 (0.023)  Loss:  0.2319 (0.1899)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/649]  Time: 0.017 (0.022)  Loss:  0.1821 (0.1864)  Acc@1: 100.0000 (99.8344)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/649]  Time: 0.018 (0.021)  Loss:  0.1739 (0.1880)  Acc@1: 100.0000 (99.7512)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/649]  Time: 0.018 (0.020)  Loss:  0.1799 (0.1902)  Acc@1: 100.0000 (99.7012)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/649]  Time: 0.018 (0.020)  Loss:  0.1925 (0.1919)  Acc@1: 100.0000 (99.5847)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/649]  Time: 0.017 (0.020)  Loss:  0.1746 (0.1907)  Acc@1: 100.0000 (99.6439)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/649]  Time: 0.018 (0.020)  Loss:  0.1979 (0.1921)  Acc@1: 100.0000 (99.6259)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/649]  Time: 0.018 (0.020)  Loss:  0.1826 (0.1920)  Acc@1: 100.0000 (99.5565)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/649]  Time: 0.018 (0.020)  Loss:  0.1637 (0.1925)  Acc@1: 100.0000 (99.5010)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/649]  Time: 0.034 (0.020)  Loss:  0.1830 (0.1927)  Acc@1: 100.0000 (99.4102)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/649]  Time: 0.018 (0.019)  Loss:  0.1671 (0.1927)  Acc@1: 100.0000 (99.3760)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 649/649]  Time: 0.015 (0.019)  Loss:  0.1338 (0.1925)  Acc@1: 100.0000 (99.3462)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/20230428-212602-dinat_base-224/checkpoint-0.pth.tar', 99.34615384615384)\n",
      "\n",
      "Train: 1 [   0/500 (  0%)]  Loss: 1.366 (1.37)  Time: 0.198s,   20.15/s  (0.198s,   20.15/s)  LR: 5.105e-05  Data: 0.051 (0.051)\n",
      "Train: 1 [  50/500 ( 10%)]  Loss: 1.085 (1.46)  Time: 0.139s,   28.70/s  (0.136s,   29.48/s)  LR: 5.604e-05  Data: 0.002 (0.003)\n",
      "Train: 1 [ 100/500 ( 20%)]  Loss: 1.563 (1.45)  Time: 0.139s,   28.73/s  (0.135s,   29.59/s)  LR: 6.104e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 150/500 ( 30%)]  Loss: 1.741 (1.44)  Time: 0.137s,   29.16/s  (0.134s,   29.78/s)  LR: 6.603e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 200/500 ( 40%)]  Loss: 1.256 (1.44)  Time: 0.130s,   30.82/s  (0.134s,   29.90/s)  LR: 7.103e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 250/500 ( 50%)]  Loss: 1.041 (1.43)  Time: 0.126s,   31.70/s  (0.134s,   29.91/s)  LR: 7.602e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 300/500 ( 60%)]  Loss: 1.082 (1.42)  Time: 0.131s,   30.49/s  (0.134s,   29.90/s)  LR: 8.102e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 350/500 ( 70%)]  Loss: 1.124 (1.43)  Time: 0.135s,   29.52/s  (0.134s,   29.90/s)  LR: 8.601e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 400/500 ( 80%)]  Loss: 1.146 (1.42)  Time: 0.142s,   28.26/s  (0.134s,   29.92/s)  LR: 9.101e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 450/500 ( 90%)]  Loss: 1.705 (1.42)  Time: 0.126s,   31.87/s  (0.134s,   29.96/s)  LR: 9.600e-05  Data: 0.002 (0.002)\n",
      "Train: 1 [ 499/500 (100%)]  Loss: 1.346 (1.42)  Time: 0.131s,   30.46/s  (0.133s,   30.15/s)  LR: 1.009e-04  Data: 0.000 (0.002)\n",
      "Test: [   0/649]  Time: 0.079 (0.079)  Loss:  0.2572 (0.2572)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/649]  Time: 0.017 (0.020)  Loss:  0.4460 (0.2875)  Acc@1: 75.0000 (99.0196)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/649]  Time: 0.019 (0.019)  Loss:  0.1393 (0.2470)  Acc@1: 100.0000 (99.2574)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/649]  Time: 0.017 (0.019)  Loss:  0.2501 (0.2125)  Acc@1: 100.0000 (99.3377)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/649]  Time: 0.017 (0.019)  Loss:  0.2668 (0.2347)  Acc@1: 100.0000 (99.1294)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/649]  Time: 0.018 (0.019)  Loss:  0.2666 (0.2477)  Acc@1: 100.0000 (99.1036)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/649]  Time: 0.018 (0.019)  Loss:  0.2757 (0.2560)  Acc@1: 100.0000 (99.0033)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/649]  Time: 0.019 (0.019)  Loss:  0.2405 (0.2590)  Acc@1: 100.0000 (99.0741)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/649]  Time: 0.017 (0.019)  Loss:  0.1271 (0.2634)  Acc@1: 100.0000 (99.0025)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/649]  Time: 0.017 (0.019)  Loss:  0.1236 (0.2495)  Acc@1: 100.0000 (99.1131)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/649]  Time: 0.019 (0.019)  Loss:  0.1213 (0.2379)  Acc@1: 100.0000 (99.2016)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/649]  Time: 0.019 (0.019)  Loss:  0.1249 (0.2301)  Acc@1: 100.0000 (99.1379)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/649]  Time: 0.018 (0.019)  Loss:  0.1226 (0.2224)  Acc@1: 100.0000 (99.1681)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 649/649]  Time: 0.015 (0.019)  Loss:  0.1103 (0.2166)  Acc@1: 100.0000 (99.1538)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/20230428-212602-dinat_base-224/checkpoint-0.pth.tar', 99.34615384615384)\n",
      " ('./output/train/20230428-212602-dinat_base-224/checkpoint-1.pth.tar', 99.15384615384616)\n",
      "\n",
      "*** Best metric: 99.34615384615384 (epoch 0)\n"
     ]
    }
   ],
   "source": [
    "!python train.py small_dog_cat_dataset --epochs 2 --batch-size 4 --model dinat_base --pretrain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vot_det",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
